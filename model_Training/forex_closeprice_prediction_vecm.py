# -*- coding: utf-8 -*-
"""Forex_ClosePrice_Prediction_vecm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uELHkJrITDVSAx775lrwHA5xgA8NUesi
"""

!pip install --upgrade scikit-learn statsmodels mplfinance

!pip install --upgrade statsmodels

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import mplfinance as mpf
from sklearn.feature_selection import mutual_info_regression



raw = pd.read_csv('/content/EURUSD_D1.csv')

raw.head()

raw.describe()

raw['Time'] = pd.to_datetime(raw['Time'], dayfirst=True)

raw.set_index('Time', inplace=True)

df = raw.resample('D').mean()
df.fillna(method='ffill', inplace=True)

df.index.freq = 'D'

df.index.freq

df

df.describe()

df.info()

df.isna().sum()

df.isnull().sum()

"""## Visualization"""

# 1. Line Chart for Closing Prices
plt.figure(figsize=(10, 6))
plt.plot(df.index, df['Close'], label='Close Price')
plt.title('Closing Prices Over Time')
plt.xlabel('Time')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show()

# Time Series Plot
plt.figure(figsize=(10, 6))
plt.plot(df.index, df['Open'], label='Open')
plt.plot(df.index, df['High'], label='High')
plt.plot(df.index, df['Low'], label='Low')
plt.plot(df.index, df['Close'], label='Close')
plt.title('Time Series of Open, High, Low, Close Prices')
plt.xlabel('Time')
plt.ylabel('Prices')
plt.legend()
plt.grid()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Subset the last 1000 rows
subset_df = df[-1000:]  # You can adjust the number of rows here

# Plot the subset of data
mpf.plot(subset_df, type='candle', volume=True, style='charles', title='Candlestick Chart (Last 1000 Rows)',figratio=(12,8), figscale=1.2)

# 3. Volume Bar Chart
plt.figure(figsize=(10, 6))
plt.bar(df.index, df['Volume'], color='purple')
plt.title('Volume Over Time')
plt.xlabel('Time')
plt.ylabel('Volume')
plt.grid(True)
plt.show()

# 5. Histogram of Close Prices
plt.figure(figsize=(10, 6))
sns.histplot(df['Close'], bins=50, kde=True)
plt.title('Histogram of Close Prices')
plt.xlabel('Close Price')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# 6. Box Plot for Price Types
plt.figure(figsize=(10, 6))
sns.boxplot(data=df[['Open', 'High', 'Low', 'Close']])
plt.title('Box Plot of Open, High, Low, Close Prices')
plt.ylabel('Price')
plt.grid(True)
plt.show()

plt.figure(figsize=(12,8))
corr = df.corr()
sns.heatmap(corr, annot=True, cmap='YlOrRd')
plt.title('Correlation Heatmap')
plt.show()

"""**Creating more features**"""

# 1. High-Low Range
df['High_Low_Range'] = df['High'] - df['Low']

# 2. Open-High Range
df['Open_High_Range'] = df['High'] - df['Open']

# 3. Open-Low Range
df['Open_Low_Range'] = df['Open'] - df['Low']

# 4. Rolling Volatility (using a 3-period window)
df['Rolling_Volatility'] = df[['Open', 'High', 'Low']].std(axis=1).rolling(window=3).mean()

# 5. True Range
df['True_Range'] = df.apply(lambda row: max(row['High'] - row['Low'], abs(row['High'] - row['Open']), abs(row['Low'] - row['Open'])), axis=1)

# 6. Moving Averages (SMA)
df['SMA_Open_3'] = df['Open'].rolling(window=3).mean()  # 3-period SMA of Open
df['SMA_High_3'] = df['High'].rolling(window=3).mean()  # 3-period SMA of High

# 7. Price Momentum (for Open and High, lag of 1 period)
df['Momentum_Open_1'] = df['Open'] - df['Open'].shift(1)
df['Momentum_High_1'] = df['High'] - df['High'].shift(1)

# 8. Exponential Moving Average (EMA) for Open (with a smoothing factor, alpha)
df['EMA_Open'] = df['Open'].ewm(span=3, adjust=False).mean()  # 3-period EMA of Open

# 9. Volume Change (percentage change)
df['Volume_Change'] = df['Volume'].pct_change() * 100

# 10. Moving Average of Volume (SMA)
df['SMA_Volume_3'] = df['Volume'].rolling(window=3).mean()

# 11. Lagged features for Open, High, Low, and Volume (lag of 1 period)
df['Lagged_Open_1'] = df['Open'].shift(1)
df['Lagged_High_1'] = df['High'].shift(1)
df['Lagged_Low_1'] = df['Low'].shift(1)
df['Lagged_Volume_1'] = df['Volume'].shift(1)

window = 14  # Common default period for RSI
delta = df['Close'].diff()
gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
df['RSI'] = 100 - (100 / (1 + gain / loss))

df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()
df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean()
df['MACD'] = df['EMA_12'] - df['EMA_26']


df['SMA_20'] = df['Close'].rolling(window=20).mean()
df['Bollinger_Upper'] = df['SMA_20'] + 2 * df['Close'].rolling(window=20).std()
df['Bollinger_Lower'] = df['SMA_20'] - 2 * df['Close'].rolling(window=20).std()


df['Trend_Direction'] = np.where(df['Close'] > df['Open'], 1, -1)


df['Returns'] = df['Close'].pct_change()
df['Lagged_Returns_1'] = df['Returns'].shift(1)


df['EMA_Volume'] = df['Volume'].ewm(span=3, adjust=False).mean()


df['Range_5'] = df['High'].rolling(window=5).max() - df['Low'].rolling(window=5).min()

df.head()

df.isnull().sum()

df.dropna(inplace=True)

df.isnull().sum()

plt.figure(figsize=(14, 10))

corr = df.corr()
ax = sns.heatmap(
    corr,
    annot=True,
    cmap='coolwarm',
    fmt='.2f',
    annot_kws={"size": 10, "weight": 'bold'},
    linewidths=0.5,
    linecolor='gray',
    cbar_kws={"shrink": .8}
)

plt.title('Correlation Heatmap', fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)

plt.tight_layout()
plt.show()

df.isnull().sum()

"""# Feature Reduction"""

import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor

def drop_multicollinear_features(df, threshold=5.0, main_features=None):
    """
    Drop features from the DataFrame that have high multicollinearity,
    excluding specified main features from the dropping process.

    Parameters:
    - df: pandas DataFrame with features.
    - threshold: VIF threshold above which features are considered collinear.
    - main_features: List of main features that should not be dropped.

    Returns:
    - DataFrame with collinear features removed.
    """

    def calculate_vif(X):
        # Ensure all data is numeric
        X = X.apply(pd.to_numeric, errors='coerce')
        # Drop rows with NaN values
        X = X.replace([np.inf, -np.inf], np.nan).dropna()

        vif = pd.DataFrame()
        vif['Features'] = X.columns
        vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
        return vif

    if main_features is None:
        main_features = []

    # Initialize the list of features to drop
    features_to_drop = []

    while True:
        # Calculate VIF for each feature
        vif = calculate_vif(df)

        # Exclude main features from dropping
        vif = vif[~vif['Features'].isin(main_features)]

        # Check for features with VIF above the threshold
        high_vif = vif[vif['VIF'] > threshold]

        if high_vif.empty:
            # If no features have high VIF, exit the loop
            break

        # Drop the feature with the highest VIF
        feature_to_drop = high_vif.sort_values('VIF', ascending=False).iloc[0]['Features']
        features_to_drop.append(feature_to_drop)
        df = df.drop(columns=[feature_to_drop])

        print(f"Dropped feature '{feature_to_drop}' with VIF: {high_vif['VIF'].max()}")

    return df, features_to_drop

# Example usage
# Assuming 'df' is your DataFrame and you want to keep these main features
main_features = ['Open', 'High', 'Low', 'Close', 'Volume']

cleaned_df, dropped_features = drop_multicollinear_features(df, threshold=5.0, main_features=main_features)

print("Dropped Features:", dropped_features)
print("Remaining Features:", cleaned_df.columns)

df.columns

cleaned_df.columns

cleaned_df.describe()

"""**PCA**"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Define the columns
pca_columns = ['High_Low_Range', 'Open_High_Range', 'Open_Low_Range',
               'Rolling_Volatility', 'True_Range', 'SMA_Open_3',
               'SMA_High_3', 'Momentum_Open_1', 'Momentum_High_1',
               'EMA_Open', 'Volume_Change', 'SMA_Volume_3',
               'Lagged_Open_1', 'Lagged_High_1', 'Lagged_Low_1',
               'Lagged_Volume_1', 'RSI', 'EMA_12', 'EMA_26',
               'MACD', 'SMA_20', 'Bollinger_Upper',
               'Bollinger_Lower', 'Trend_Direction',
               'Returns', 'Lagged_Returns_1', 'EMA_Volume',
               'Range_5']

pca_data = df[pca_columns]
original_data = df[['Open', 'High', 'Low', 'Close', 'Volume']]

# Standardize the data
scaler = StandardScaler()
pca_data_scaled = scaler.fit_transform(pca_data)

pca = PCA()
pca_components = pca.fit_transform(pca_data_scaled)

# Create descriptive names for the components
n_components = pca.n_components_
component_names = [f'Principal Component {i+1}' for i in range(n_components)]

# Create a DataFrame with PCA components and descriptive names
pca_df = pd.DataFrame(data=pca_components, columns=component_names)

# Plot explained variance ratio
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')
plt.title('Explained Variance by Principal Components')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.grid()
plt.show()

n_components_to_keep = 6
reduced_pca_df = pd.DataFrame(data=pca_df.iloc[:, :n_components_to_keep])

# Combine reduced PCA components with original data
combined_reduced_df = pd.concat([df[['Open', 'High', 'Low', 'Close', 'Volume']].reset_index(drop=False),
                                   reduced_pca_df.reset_index(drop=True)], axis=1)

# If 'Time' is a column in your DataFrame
combined_reduced_df['Time'] = pd.to_datetime(combined_reduced_df['Time'])
combined_reduced_df.set_index('Time', inplace=True)

combined_reduced_df.head()

combined_reduced_df.describe()

plt.figure(figsize=(14, 10))

corr = combined_reduced_df.corr()
ax = sns.heatmap(
    corr,
    annot=True,
    cmap='coolwarm',
    fmt='.2f',
    annot_kws={"size": 10, "weight": 'bold'},
    linewidths=0.5,
    linecolor='gray',
    cbar_kws={"shrink": .8}
)

plt.title('Correlation Heatmap', fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)

plt.tight_layout()
plt.show()



"""## Splitting the Data"""

combined_reduced_df = combined_reduced_df.sort_index()

data = combined_reduced_df

# std_scaler = StandardScaler()

# data['Volume'] = std_scaler.fit_transform(data[['Volume']])

# Split into training (80%) and testing (20%) sets based on time
train_size = int(len(combined_reduced_df) * 0.8)
X_train = combined_reduced_df.iloc[:train_size]
X_test = combined_reduced_df.iloc[train_size:]

# Display shapes of training and testing sets
print(f'Training set shape: {X_train.shape}')
print(f'Testing set shape: {X_test.shape}')

data = data.asfreq('D')

print("Current Frequency:", data.index.freq)

X_train.head()

X_train.tail()

X_train.isnull().sum()

"""## Training and Evaluating Models

### VECM Model
"""

from statsmodels.tsa.vector_ar.vecm import *

"""### Johansen Test for Cointegration"""

import pandas as pd
import numpy as np
from statsmodels.tsa.vector_ar.vecm import coint_johansen

def test_cointegration(df):
    """
    Perform Johansen cointegration test on level data.

    Parameters:
    - df: pandas DataFrame with level time series data.

    Returns:
    - None; prints the results.
    """
    # Check for missing values
    if df.isnull().sum().sum() > 0:
        df = df.fillna(method='ffill').fillna(method='bfill')  # Forward and backward fill

    # Check if the matrix is positive definite
    try:
        jres = coint_johansen(df, det_order=1, k_ar_diff=1)  # Adjust det_order as needed
        trace_stat = jres.lr1
        crit_values = jres.cvt
        print('Johansen Cointegration Test Results:')
        print('Trace Statistic:', trace_stat)
        print('Critical Values:', crit_values)

        num_cointegrating_relations = np.sum(trace_stat > crit_values[:, 1])
        print(f'Number of Cointegrating Relations: {num_cointegrating_relations}')

    except np.linalg.LinAlgError as e:
        print("Error during Johansen test:", e)

test_cointegration(X_train)

"""### Phillips-Hansen Cointegration Test"""

from statsmodels.tsa.stattools import coint

def test_cointegration_phillips_hansen(df):
    for column in df.columns:
        for other_column in df.columns:
            if column != other_column:
                score, p_value, _ = coint(df[column], df[other_column])
                print(f'Cointegration test between {column} and {other_column}: p-value = {p_value}')

# test_cointegration_phillips_hansen(data)

# Split the data into training and test sets (e.g., 80% training, 20% test)
train_size = int(0.8 * len(data))
train_data = data[:train_size]
test_data = data[train_size:]

test_data.head()

# Select lag order and cointegration rank based on the training data
lag_order_train = select_order(data=train_data, maxlags=10, deterministic="ci", seasons=5)
rank_test_train = select_coint_rank(train_data, 1, 1, method="trace", signif=0.1)

# Fit VECM model on the training data
vecm_model = VECM(train_data, deterministic="ci", seasons=5,
                  k_ar_diff=lag_order.aic,
                  coint_rank=rank_test.rank)
vecm_res = vecm_model.fit()

n_steps = 5
vecm_res.predict(steps=n_steps)

train_data.head()

forecast = vecm_res.predict(steps=n_steps)


forecast_interval = vecm_res.predict(steps=n_steps, alpha=0.05)
lower_bound, upper_bound = forecast_interval[1], forecast_interval[2]

forecast_index = pd.date_range(start=train_data.index[-1] + pd.Timedelta(days=1), periods=n_steps, freq='B')
forecast_df = pd.DataFrame(forecast, index=forecast_index, columns=train_data.columns)

test_data.head(n_steps)

forecast_df.head(n_steps)

"""**Testing**"""

# Assuming 'test_data' is your DataFrame with testing data
# Split the last 20 observations
actual_last_20 = test_data.iloc[:n_steps]
remaining_test_data = data.iloc[:-n_steps]  # All but the last 20

def calculate_metrics(actual, predicted):
    mae = np.mean(np.abs(actual - predicted))
    mse = np.mean((actual - predicted) ** 2)
    rmse = np.sqrt(mse)
    mape = np.mean(np.abs((actual - predicted) / actual)) * 100  # MAPE in percentage

    return mae, mse, rmse, mape


# Ensure we only evaluate columns that exist in both DataFrames
common_columns = set(actual_last_20.columns).intersection(set(forecast_df.columns))

# Loop through each common column and calculate metrics
for column in common_columns:
    if column not in  ['Principal Component 0','Principal Component 1','Principal Component 2','Principal Component 3',
                    'Principal Component 4','Principal Component 5','Principal Component 6','Principal Component 7']:
      actual_values = actual_last_20[column][:n_steps]

      # Get predicted values from forecast_df
      predicted_values = forecast_df[column]

      # Calculate metrics
      mae, mse, rmse, mape = calculate_metrics(actual_values, predicted_values)

      # Print results
      print(f"\nEvaluation Metrics for '{column}':")
      print(f"Mean Absolute Error (MAE): {mae:.8f}")
      print(f"Mean Squared Error (MSE): {mse:.8f}")
      print(f"Root Mean Squared Error (RMSE): {rmse:.8f}")
      print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")

# Identify common columns
common_columns = set(actual_last_20.columns).intersection(set(forecast_df.columns))

# Prepare the data for plotting by extracting actual and forecasted values for common columns
plot_data = {}
for column in common_columns:
  if column not in ['Principal Component 0','Principal Component 1','Principal Component 2','Principal Component 3',
                    'Principal Component 4','Principal Component 5','Principal Component 6','Principal Component 7']:
    actual_values = actual_last_20[column]
    forecasted_values = forecast_df[column]
    plot_data[column] = (actual_values, forecasted_values)

# Create a figure for plotting
fig, axs = plt.subplots(nrows=len(plot_data), ncols=1, figsize=(14, 7 * len(plot_data)), sharex=True)

# Loop through each common column and create subplots
for i, (column, (actual, forecasted)) in enumerate(plot_data.items()):
  if column not in  ['Principal Component 0','Principal Component 1','Principal Component 2','Principal Component 3',
                    'Principal Component 4','Principal Component 5','Principal Component 6','Principal Component 7']:
    axs[i].plot(actual.index, actual, label='Actual', color='blue', marker='o')  # Actual values
    axs[i].plot(forecasted.index, forecasted, label='Forecasted', color='orange', linestyle='--', marker='x')  # Forecasted values
    axs[i].set_title(f'Actual vs Forecasted: {column}')
    axs[i].set_ylabel(column)
    axs[i].legend()
    axs[i].grid()

# Set common x-label for all subplots
axs[-1].set_xlabel('Date')
plt.tight_layout()
plt.show()



"""## Structural analysis

**Granger causality**
"""

granger_results = vecm_res.test_granger_causality(caused="Close", signif=0.1)
granger_results.summary()

granger_results = vecm_res.test_granger_causality(caused="Volume", signif=0.1)
granger_results.summary()

granger_results = vecm_res.test_granger_causality(caused="Open", signif=0.1)
granger_results.summary()

"""#### Instantaneous causality"""

print(test_data.columns)

inst_caus_dp_r = vecm_res.test_inst_causality(causing='Close')
inst_caus_r_dp = vecm_res.test_inst_causality(causing="Volume")
inst_caus_r_dp.summary()

inst_caus_dp_r = vecm_res.test_inst_causality(causing="Volume")
inst_caus_r_dp = vecm_res.test_inst_causality(causing="Close")
inst_caus_r_dp.summary()

"""**Impulse-Response-Analysis**"""

num_periods = 10
ir = vecm_res.irf(periods=num_periods)
plt.clf()
ir.plot(impulse='Close',plot_stderr=False)

num_periods = 10
ir = vecm_res.irf(periods=num_periods)
plt.clf()
ir.plot(impulse='Close', response='High', orth=True, plot_stderr=False)

