# -*- coding: utf-8 -*-
"""Forex_ClosePrice_Prediction_var.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CARKyjI3SFysY2tOMKECZ2cgligKh_zN
"""

!pip install --upgrade scikit-learn statsmodels mplfinance

!pip install --upgrade statsmodels

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import mplfinance as mpf
from sklearn.feature_selection import mutual_info_regression



raw = pd.read_csv('/content/EURUSD_D1.csv')

raw.head()

raw['Time'] = pd.to_datetime(raw['Time'], dayfirst=True)

raw.set_index('Time', inplace=True)

df = raw.resample('D').mean()
df.fillna(method='ffill', inplace=True)

df.index.freq = 'D'

df.index.freq

df

df.describe()

df.info()

df.isna().sum()

df.isnull().sum()

"""## Visualization"""

# 1. Line Chart for Closing Prices
plt.figure(figsize=(10, 6))
plt.plot(df.index, df['Close'], label='Close Price')
plt.title('Closing Prices Over Time')
plt.xlabel('Time')
plt.ylabel('Close Price')
plt.legend()
plt.grid(True)
plt.show()

# Time Series Plot
plt.figure(figsize=(10, 6))
plt.plot(df.index, df['Open'], label='Open')
plt.plot(df.index, df['High'], label='High')
plt.plot(df.index, df['Low'], label='Low')
plt.plot(df.index, df['Close'], label='Close')
plt.title('Time Series of Open, High, Low, Close Prices')
plt.xlabel('Time')
plt.ylabel('Prices')
plt.legend()
plt.grid()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Subset the last 1000 rows
subset_df = df[-1000:]  # You can adjust the number of rows here

# Plot the subset of data
mpf.plot(subset_df, type='candle', volume=True, style='charles', title='Candlestick Chart (Last 1000 Rows)',figratio=(12,8), figscale=1.2)

# 3. Volume Bar Chart
plt.figure(figsize=(10, 6))
plt.bar(df.index, df['Volume'], color='purple')
plt.title('Volume Over Time')
plt.xlabel('Time')
plt.ylabel('Volume')
plt.grid(True)
plt.show()

# 5. Histogram of Close Prices
plt.figure(figsize=(10, 6))
sns.histplot(df['Close'], bins=50, kde=True)
plt.title('Histogram of Close Prices')
plt.xlabel('Close Price')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# 6. Box Plot for Price Types
plt.figure(figsize=(10, 6))
sns.boxplot(data=df[['Open', 'High', 'Low', 'Close']])
plt.title('Box Plot of Open, High, Low, Close Prices')
plt.ylabel('Price')
plt.grid(True)
plt.show()

plt.figure(figsize=(12,8))
corr = df.corr()
sns.heatmap(corr, annot=True, cmap='YlOrRd')
plt.title('Correlation Heatmap')
plt.show()

"""**Creating more features**"""

# 1. High-Low Range
df['High_Low_Range'] = df['High'] - df['Low']

# 2. Open-High Range
df['Open_High_Range'] = df['High'] - df['Open']

# 3. Open-Low Range
df['Open_Low_Range'] = df['Open'] - df['Low']

# 4. Rolling Volatility (using a 3-period window)
df['Rolling_Volatility'] = df[['Open', 'High', 'Low']].std(axis=1).rolling(window=3).mean()

# 5. True Range
df['True_Range'] = df.apply(lambda row: max(row['High'] - row['Low'], abs(row['High'] - row['Open']), abs(row['Low'] - row['Open'])), axis=1)

# 6. Moving Averages (SMA)
df['SMA_Open_3'] = df['Open'].rolling(window=3).mean()  # 3-period SMA of Open
df['SMA_High_3'] = df['High'].rolling(window=3).mean()  # 3-period SMA of High

# 7. Price Momentum (for Open and High, lag of 1 period)
df['Momentum_Open_1'] = df['Open'] - df['Open'].shift(1)
df['Momentum_High_1'] = df['High'] - df['High'].shift(1)

# 8. Exponential Moving Average (EMA) for Open (with a smoothing factor, alpha)
df['EMA_Open'] = df['Open'].ewm(span=3, adjust=False).mean()  # 3-period EMA of Open

# 9. Volume Change (percentage change)
df['Volume_Change'] = df['Volume'].pct_change() * 100

# 10. Moving Average of Volume (SMA)
df['SMA_Volume_3'] = df['Volume'].rolling(window=3).mean()

# 11. Lagged features for Open, High, Low, and Volume (lag of 1 period)
df['Lagged_Open_1'] = df['Open'].shift(1)
df['Lagged_High_1'] = df['High'].shift(1)
df['Lagged_Low_1'] = df['Low'].shift(1)
df['Lagged_Volume_1'] = df['Volume'].shift(1)

window = 14  # Common default period for RSI
delta = df['Close'].diff()
gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
df['RSI'] = 100 - (100 / (1 + gain / loss))

df['EMA_12'] = df['Close'].ewm(span=12, adjust=False).mean()
df['EMA_26'] = df['Close'].ewm(span=26, adjust=False).mean()
df['MACD'] = df['EMA_12'] - df['EMA_26']


df['SMA_20'] = df['Close'].rolling(window=20).mean()
df['Bollinger_Upper'] = df['SMA_20'] + 2 * df['Close'].rolling(window=20).std()
df['Bollinger_Lower'] = df['SMA_20'] - 2 * df['Close'].rolling(window=20).std()


df['Trend_Direction'] = np.where(df['Close'] > df['Open'], 1, -1)


df['Returns'] = df['Close'].pct_change()
df['Lagged_Returns_1'] = df['Returns'].shift(1)


df['EMA_Volume'] = df['Volume'].ewm(span=3, adjust=False).mean()


df['Range_5'] = df['High'].rolling(window=5).max() - df['Low'].rolling(window=5).min()

df.head()

df.isnull().sum()

df.dropna(inplace=True)

df.isnull().sum()

plt.figure(figsize=(14, 10))

corr = df.corr()
ax = sns.heatmap(
    corr,
    annot=True,
    cmap='coolwarm',
    fmt='.2f',
    annot_kws={"size": 10, "weight": 'bold'},
    linewidths=0.5,
    linecolor='gray',
    cbar_kws={"shrink": .8}
)

plt.title('Correlation Heatmap', fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)

plt.tight_layout()
plt.show()

df.isnull().sum()

"""# Feature Reduction"""

import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor

def drop_multicollinear_features(df, threshold=5.0, main_features=None):
    """
    Drop features from the DataFrame that have high multicollinearity,
    excluding specified main features from the dropping process.

    Parameters:
    - df: pandas DataFrame with features.
    - threshold: VIF threshold above which features are considered collinear.
    - main_features: List of main features that should not be dropped.

    Returns:
    - DataFrame with collinear features removed.
    """

    def calculate_vif(X):
        # Ensure all data is numeric
        X = X.apply(pd.to_numeric, errors='coerce')
        # Drop rows with NaN values
        X = X.replace([np.inf, -np.inf], np.nan).dropna()

        vif = pd.DataFrame()
        vif['Features'] = X.columns
        vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
        return vif

    if main_features is None:
        main_features = []

    # Initialize the list of features to drop
    features_to_drop = []

    while True:
        # Calculate VIF for each feature
        vif = calculate_vif(df)

        # Exclude main features from dropping
        vif = vif[~vif['Features'].isin(main_features)]

        # Check for features with VIF above the threshold
        high_vif = vif[vif['VIF'] > threshold]

        if high_vif.empty:
            # If no features have high VIF, exit the loop
            break

        # Drop the feature with the highest VIF
        feature_to_drop = high_vif.sort_values('VIF', ascending=False).iloc[0]['Features']
        features_to_drop.append(feature_to_drop)
        df = df.drop(columns=[feature_to_drop])

        print(f"Dropped feature '{feature_to_drop}' with VIF: {high_vif['VIF'].max()}")

    return df, features_to_drop

# Example usage
# Assuming 'df' is your DataFrame and you want to keep these main features
main_features = ['Open', 'High', 'Low', 'Close', 'Volume']

cleaned_df, dropped_features = drop_multicollinear_features(df, threshold=5.0, main_features=main_features)

print("Dropped Features:", dropped_features)
print("Remaining Features:", cleaned_df.columns)

df.columns

cleaned_df.columns

cleaned_df.describe()

"""**FEATURE REDUCTION USING PCA**"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Define the columns
pca_columns = ['High_Low_Range', 'Open_High_Range', 'Open_Low_Range',
               'Rolling_Volatility', 'True_Range', 'SMA_Open_3',
               'SMA_High_3', 'Momentum_Open_1', 'Momentum_High_1',
               'EMA_Open', 'Volume_Change', 'SMA_Volume_3',
               'Lagged_Open_1', 'Lagged_High_1', 'Lagged_Low_1',
               'Lagged_Volume_1', 'RSI', 'EMA_12', 'EMA_26',
               'MACD', 'SMA_20', 'Bollinger_Upper',
               'Bollinger_Lower', 'Trend_Direction',
               'Returns', 'Lagged_Returns_1', 'EMA_Volume',
               'Range_5']

pca_data = df[pca_columns]
original_data = df[['Open', 'High', 'Low', 'Close', 'Volume']]

# Standardize the data
scaler = StandardScaler()
pca_data_scaled = scaler.fit_transform(pca_data)

pca = PCA()
pca_components = pca.fit_transform(pca_data_scaled)

# Create descriptive names for the components
n_components = pca.n_components_
component_names = [f'Principal Component {i+1}' for i in range(n_components)]

# Create a DataFrame with PCA components and descriptive names
pca_df = pd.DataFrame(data=pca_components, columns=component_names)

# Plot explained variance ratio
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')
plt.title('Explained Variance by Principal Components')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.grid()
plt.show()

n_components_to_keep = 6
reduced_pca_df = pd.DataFrame(data=pca_df.iloc[:, :n_components_to_keep])

# Combine reduced PCA components with original data
combined_reduced_df = pd.concat([df[['Open', 'High', 'Low', 'Close', 'Volume']].reset_index(drop=False),
                                   reduced_pca_df.reset_index(drop=True)], axis=1)

# If 'Time' is a column in your DataFrame
combined_reduced_df['Time'] = pd.to_datetime(combined_reduced_df['Time'])
combined_reduced_df.set_index('Time', inplace=True)

plt.figure(figsize=(14, 10))

corr = combined_reduced_df.corr()
ax = sns.heatmap(
    corr,
    annot=True,
    cmap='coolwarm',
    fmt='.2f',
    annot_kws={"size": 10, "weight": 'bold'},
    linewidths=0.5,
    linecolor='gray',
    cbar_kws={"shrink": .8}
)

plt.title('Correlation Heatmap', fontsize=16)
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)

plt.tight_layout()
plt.show()

"""**Splitting Data**"""

# Define the split ratio
train_size = 0.8  # 80% for training, 20% for testing

# Calculate the index for splitting
train_index = int(len(combined_reduced_df) * train_size)

# Split the DataFrame into training and testing sets
for_training = combined_reduced_df.iloc[:train_index]  # Training set
X_test = combined_reduced_df.iloc[train_index:]   # Testing set

# Display the shapes of the resulting DataFrames
print(f"Training set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")

"""## Correcting Non-stationary behaviour"""

import pandas as pd
from statsmodels.tsa.stattools import adfuller

def differencing(df, columns=None, max_differences=2):
    """
    Apply differencing to correct non-stationary nature of specified columns in a DataFrame.

    Parameters:
    - df: pandas DataFrame with time series data.
    - columns: List of columns to check and difference. If None, all columns are processed.
    - max_differences: Maximum number of differencing operations to apply.

    Returns:
    - DataFrame with differenced columns.
    """

    if columns is None:
        columns = df.columns

    def check_stationarity(series):
        """
        Check if a series is stationary using the ADF test.
        """
        result = adfuller(series.dropna())
        return result[1] < 0.05  # p-value < 0.05 indicates stationarity

    def apply_differencing(series, max_differences):
        """
        Apply differencing to a series until it is stationary or the max number of differences is reached.
        """
        differenced_series = series.copy()
        for i in range(max_differences):
            if check_stationarity(differenced_series):
                break
            differenced_series = differenced_series.diff().dropna()
        return differenced_series

    # Create a copy of the DataFrame to avoid modifying the original data
    df_differenced = df.copy()

    # Apply differencing to each specified column
    for col in columns:
        if not check_stationarity(df[col]):
            print(f"Column '{col}' is non-stationary. Applying differencing...")
            df_differenced[col] = apply_differencing(df[col], max_differences)
        else:
            print(f"Column '{col}' is already stationary.")

    return df_differenced

# Assuming 'df' is your DataFrame with time series data
X_train = differencing(for_training)

X_train = differencing(X_train)

X_train

X_train.isnull().sum()

X_train.dropna(inplace=True)

X_train.isna().sum()

"""## Splitting the Data"""

X_train = X_train.asfreq('D')

print("Current Frequency:", X_train.index.freq)

X_train.describe()

"""## Training and Evaluating Models"""

from statsmodels.tsa.api import VAR
from statsmodels.tsa.vector_ar.vecm import VECM
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.vector_ar.hypothesis_test_results import CausalityTestResults
from statsmodels.tsa.vector_ar.irf import IRAnalysis

"""### VAR MODEL

#### Selecting the Best Hyperparameters (Optimal Lags)
"""

def find_best_lag(df, max_lags=15):
    """
    Find the best lag order manually by fitting VAR models with different lags.

    Parameters:
    - df: pandas DataFrame with time series data.
    - max_lags: maximum number of lags to consider.

    Returns:
    - best_lag: optimal number of lags based on lowest AIC.
    """
    aic_values = []
    bic_values = []

    for lag in range(1, max_lags + 1):
        model = VAR(df)
        try:
            model_fit = model.fit(lag)
            aic = model_fit.aic
            bic = model_fit.bic
            aic_values.append(aic)
            bic_values.append(bic)
        except Exception as e:
            print(f"Error fitting VAR model with lag {lag}: {e}")
            aic_values.append(float('inf'))
            bic_values.append(float('inf'))

    best_lag = aic_values.index(min(aic_values)) + 1
    print(f"Best lag based on AIC: {best_lag}")
    return best_lag

optimal_lags = find_best_lag(X_train, max_lags=20)

"""**Testing**"""

print(optimal_lags)

# Fit the VAR model
model = VAR(X_train)

model_fit = model.fit(maxlags=optimal_lags)

# Display summary of the model fit
model_fit.summary()

n_steps = 20
forecast = model_fit.forecast(y=X_train.values[-model_fit.k_ar:], steps=n_steps)

# Create a DataFrame for the forecast
forecast_index = pd.date_range(start=X_train.index[-1] + pd.Timedelta(days=1), periods=n_steps, freq='B')
forecast_df = pd.DataFrame(forecast, index=forecast_index, columns=X_train.columns)

confidence_interval_percentage = 0.05
historical_errors = model_fit.resid.std(axis=0)
historical_errors_expanded = np.tile(historical_errors, (forecast.shape[0], 1))

# Calculate lower and upper bounds
lower_bound = forecast - (confidence_interval_percentage * historical_errors_expanded)
upper_bound = forecast + (confidence_interval_percentage * historical_errors_expanded)

# Print the forecast and bounds
print("Forecast:")
forecast_df.head()

print("\nLower Bound:")
print(lower_bound)

print("\nUpper Bound:")
print(upper_bound)

last_actual_values = for_training.iloc[-1].values  # Get the last row of X_train

# Step 2: Inverse differencing
# Calculate cumulative sum of forecasted differences
forecast_cumsum = np.cumsum(forecast_df.values, axis=0)

# Step 3: Add last actual values to get predictions in original scale
inverse_forecast = forecast_cumsum + last_actual_values

# Create a DataFrame for inverse forecast
inverse_forecast_df = pd.DataFrame(inverse_forecast, index=forecast_index, columns=forecast_df.columns)

inverse_forecast_df.head()

# Assuming 'test_data' is your DataFrame with testing data
# Split the last 20 observations
actual_last_20 = X_test.iloc[:n_steps]
remaining_test_data = X_test.iloc[:-n_steps]  # All but the last 20

def calculate_metrics(actual, predicted):
    mae = np.mean(np.abs(actual - predicted))
    mse = np.mean((actual - predicted) ** 2)
    rmse = np.sqrt(mse)
    mape = np.mean(np.abs((actual - predicted) / actual)) * 100  # MAPE in percentage

    return mae, mse, rmse, mape


# Ensure we only evaluate columns that exist in both DataFrames
common_columns = set(actual_last_20.columns).intersection(set(inverse_forecast_df.columns))

# Loop through each common column and calculate metrics
for column in common_columns:
    if column not in  ['Principal Component 0','Principal Component 1','Principal Component 2','Principal Component 3',
                    'Principal Component 4','Principal Component 5','Principal Component 6','Principal Component 7']:
      actual_values = actual_last_20[column][:n_steps]

      # Get predicted values from forecast_df
      predicted_values = inverse_forecast_df[column]

      # Calculate metrics
      mae, mse, rmse, mape = calculate_metrics(actual_values, predicted_values)

      # Print results
      print(f"\nEvaluation Metrics for '{column}':")
      print(f"Mean Absolute Error (MAE): {mae:.8f}")
      print(f"Mean Squared Error (MSE): {mse:.8f}")
      print(f"Root Mean Squared Error (RMSE): {rmse:.8f}")
      print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")

# Identify common columns
common_columns = set(actual_last_20.columns).intersection(set(inverse_forecast_df.columns))

# Prepare the data for plotting by extracting actual and forecasted values for common columns
plot_data = {}
for column in common_columns:
  if column not in ['Principal Component 0','Principal Component 1','Principal Component 2','Principal Component 3',
                    'Principal Component 4','Principal Component 5','Principal Component 6','Principal Component 7']:
    actual_values = actual_last_20[column]
    forecasted_values = inverse_forecast_df[column]
    plot_data[column] = (actual_values, forecasted_values)

# Create a figure for plotting
fig, axs = plt.subplots(nrows=len(plot_data), ncols=1, figsize=(14, 7 * len(plot_data)), sharex=True)

# Loop through each common column and create subplots
for i, (column, (actual, forecasted)) in enumerate(plot_data.items()):
  if column not in  ['Principal Component 0','Principal Component 1','Principal Component 2','Principal Component 3',
                    'Principal Component 4','Principal Component 5','Principal Component 6','Principal Component 7']:
    axs[i].plot(actual.index, actual, label='Actual', color='blue', marker='o')  # Actual values
    axs[i].plot(forecasted.index, forecasted, label='Forecasted', color='orange', linestyle='--', marker='x')  # Forecasted values
    axs[i].set_title(f'Actual vs Forecasted: {column}')
    axs[i].set_ylabel(column)
    axs[i].legend()
    axs[i].grid()

# Set common x-label for all subplots
axs[-1].set_xlabel('Date')
plt.tight_layout()
plt.show()